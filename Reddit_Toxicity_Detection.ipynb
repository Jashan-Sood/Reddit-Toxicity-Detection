{
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "smagnan_1_million_reddit_comments_from_40_subreddits_path = kagglehub.dataset_download('smagnan/1-million-reddit-comments-from-40-subreddits')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "yFlFjjaudxSx"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "zqo6m86FdxS1"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries\n",
        "!pip install transformers datasets torch tqdm seaborn matplotlib plotly networkx wordcloud\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "C5RlnQ6xfG1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/root/.cache/kagglehub/datasets/smagnan/1-million-reddit-comments-from-40-subreddits/versions/1/kaggle_RC_2019-05.csv')\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "8fQ-4UvNfH7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop null or deleted comments\n",
        "data = data.dropna(subset=['body'])\n",
        "data = data[data['body'].str.lower().ne('[deleted]')]\n",
        "data = data[data['body'].str.lower().ne('[removed]')]\n",
        "\n",
        "# Text cleaning\n",
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)         # remove URLs\n",
        "    text = re.sub(r\"@\\S+\", \"\", text)            # remove mentions\n",
        "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)  # remove punctuation\n",
        "    text = re.sub(r\"\\s+\", \" \", text)            # normalize spaces\n",
        "    return text.strip().lower()\n",
        "\n",
        "data['clean_body'] = data['body'].apply(clean_text)\n",
        "\n",
        "print(f\"Dataset size after cleaning: {len(data)} rows\")"
      ],
      "metadata": {
        "id": "KSQXLI32gHko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Use pretrained toxicity model from Hugging Face\n",
        "toxic_detector = pipeline(\"text-classification\", model=\"unitary/toxic-bert\", truncation=True, device=0)\n",
        "\n",
        "# Sample a manageable subset for GPU memory\n",
        "sample_data = data.sample(2000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Predict toxicity\n",
        "tqdm.pandas()\n",
        "sample_data['toxicity_output'] = sample_data['clean_body'].progress_apply(lambda x: toxic_detector(x)[0])\n",
        "sample_data['toxicity_label'] = sample_data['toxicity_output'].apply(lambda x: x['label'])\n",
        "sample_data['toxicity_score'] = sample_data['toxicity_output'].apply(lambda x: x['score'])\n"
      ],
      "metadata": {
        "id": "2WaRbwpggsoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(sample_data['toxicity_score'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Toxicity Scores (sample)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ODinoIrdh84v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_by_subreddit = sample_data.groupby('subreddit')['toxicity_score'].mean().sort_values(ascending=False).head(10)\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(x=toxic_by_subreddit.values, y=toxic_by_subreddit.index, palette=\"Reds_r\")\n",
        "plt.title(\"Top 10 Subreddits by Average Toxicity\")\n",
        "plt.xlabel(\"Average Toxicity Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B8k3iTi7h9ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=sample_data, x='controversiality', y='toxicity_score', alpha=0.6)\n",
        "plt.title(\"Toxicity vs Controversiality\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "z3sOI9f-iCX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(data=sample_data, x='score', y='toxicity_score', alpha=0.5)\n",
        "plt.xscale('symlog')\n",
        "plt.title(\"Community Score vs Toxicity\")\n",
        "plt.xlabel(\"Upvotes (score)\")\n",
        "plt.ylabel(\"Toxicity\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T8NSALjHiGMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "toxic_comments = ' '.join(sample_data[sample_data['toxicity_label'] == 'toxic']['clean_body'])\n",
        "wordcloud = WordCloud(width=1000, height=500, background_color='black').generate(toxic_comments)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"Most Frequent Words in Toxic Comments\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BOsg32cQiJ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute subreddit-level metrics\n",
        "subreddit_stats = (\n",
        "    sample_data.groupby('subreddit')\n",
        "    .agg({\n",
        "        'toxicity_score': ['mean', 'std'],\n",
        "        'score': 'mean',\n",
        "        'controversiality': 'mean',\n",
        "        'body': 'count'\n",
        "    })\n",
        ")\n",
        "subreddit_stats.columns = ['avg_toxicity', 'std_toxicity', 'avg_score', 'avg_controversiality', 'comment_count']\n",
        "subreddit_stats = subreddit_stats.reset_index()\n",
        "\n",
        "# Correlations\n",
        "corr = subreddit_stats[['avg_toxicity', 'avg_score', 'avg_controversiality', 'comment_count']].corr()\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Matrix: Toxicity and Community Attributes\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TE5Rclc8lWzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(\n",
        "    data=subreddit_stats,\n",
        "    x='avg_score',\n",
        "    y='avg_toxicity',\n",
        "    size='comment_count',\n",
        "    hue='avg_controversiality',\n",
        "    alpha=0.7,\n",
        "    palette='coolwarm'\n",
        ")\n",
        "plt.title(\"Subreddit Toxicity vs Community Engagement\")\n",
        "plt.xlabel(\"Average Community Score (Upvotes)\")\n",
        "plt.ylabel(\"Average Toxicity\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vO2Fm8DnlziA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = subreddit_stats[['subreddit', 'avg_toxicity', 'avg_score', 'avg_controversiality', 'comment_count']].sort_values('avg_toxicity', ascending=False).head(10)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "1CrH6rGCl2LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Normalize the numeric features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(subreddit_stats[['avg_toxicity', 'avg_score', 'avg_controversiality']])\n",
        "\n",
        "# Compute pairwise cosine similarity between subreddits\n",
        "similarity_matrix = cosine_similarity(X_scaled)\n",
        "similarity_df = pd.DataFrame(similarity_matrix,\n",
        "                             index=subreddit_stats['subreddit'],\n",
        "                             columns=subreddit_stats['subreddit'])\n"
      ],
      "metadata": {
        "id": "4xPJWa4wmEhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes\n",
        "for sub in subreddit_stats['subreddit']:\n",
        "    G.add_node(sub)\n",
        "\n",
        "# Add edges based on similarity threshold\n",
        "threshold = 0.7\n",
        "for i, sub_i in enumerate(subreddit_stats['subreddit']):\n",
        "    for j, sub_j in enumerate(subreddit_stats['subreddit']):\n",
        "        if i < j and similarity_df.iloc[i, j] > threshold:\n",
        "            G.add_edge(sub_i, sub_j, weight=similarity_df.iloc[i, j])\n"
      ],
      "metadata": {
        "id": "GHLzu9xQpAtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic metrics\n",
        "num_nodes = G.number_of_nodes()\n",
        "num_edges = G.number_of_edges()\n",
        "density = nx.density(G)\n",
        "avg_degree = sum(dict(G.degree()).values()) / num_nodes\n",
        "\n",
        "print(f\"Nodes: {num_nodes}, Edges: {num_edges}, Density: {density:.3f}, Avg Degree: {avg_degree:.2f}\")\n",
        "\n",
        "# Centrality measures\n",
        "centrality = nx.degree_centrality(G)\n",
        "betweenness = nx.betweenness_centrality(G)\n",
        "closeness = nx.closeness_centrality(G)\n"
      ],
      "metadata": {
        "id": "a77xNwR1peeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, k=0.5, seed=42)\n",
        "nx.draw_networkx_nodes(G, pos, node_size=800, node_color='skyblue', alpha=0.8)\n",
        "nx.draw_networkx_edges(G, pos, width=1, alpha=0.5)\n",
        "nx.draw_networkx_labels(G, pos, font_size=9, font_weight='bold')\n",
        "\n",
        "plt.title(\"Subreddit Co-Toxicity Network\", fontsize=14)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l12pPsxNphVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import community.community_louvain as community_louvain\n",
        "\n",
        "# Perform Louvain clustering\n",
        "partition = community_louvain.best_partition(G, weight='weight')\n",
        "\n",
        "# Add cluster labels to your data\n",
        "subreddit_stats['cluster'] = subreddit_stats['subreddit'].map(partition)\n",
        "\n",
        "# Visualize clusters\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, k=0.5, seed=42)\n",
        "colors = [partition[node] for node in G.nodes()]\n",
        "nx.draw_networkx(G, pos, node_color=colors, with_labels=True, cmap=plt.cm.Set3, node_size=800, font_size=8)\n",
        "plt.title(\"Community Clusters Based on Co-Toxicity\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9N0xmDiopj3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JOdc44mWqCZe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}